<date>June 15, 2018</date>
<h2>Kubernetes Basics</h2>
<p>
    Container orchestration is quickly becoming a defacto standard among tech firms.
    As software requirements become more and more complex and service outages are 
    increasingly expensive, the systems we build have to scale up seamlessly and be 
    highly available.
</p>
<p>
    Kubernetes or <b>k8s</b> is one possible tool to fulfill that job. In it's own
    words:
</p>
<blockquote>
    Kubernetes is an open-source system for automating deployment, scaling, and 
    management of containerized applications.
</blockquote>
<p>
    At the core of k8s there are a few abstractions that we have a look
    at to get started with orchestrating and automating our containers.
</p>
<ul>
    <li>Pods<sup>1</sup></li>
    <li>Deployments<sup>2</sup></li>
    <li>Services<sup>3</sup></li>
    <li>Ingress<sup>4</sup></li>
</ul>
<p>
    These concepts allow us to engineer products that are resilient
    and self healing in a concise manner.
</p>

<h3>Manifests</h3>
<p>
    The resources necessary for k8s are handled by <b>kubectl</b> - either manually 
    or via manifest files. For automation purposes manifest files are best 
    practice. All Manifest files follow the same structure and can be interpreted
    in multiple formats. Here an example in yaml
</p>
<pre><code class="less">apiVersion: v1
kind: Pod
metadata:
    name: podname
    namespace: ns
    labels:
        key1: value1
        key2: value2
spec:
    ...
</code></pre>
<p>
    The following base elements are specified for each manifest file
</p>
<ul>
    <li><b>apiVersion</b> - The version of the k8s API</li>
    <li><b>kind</b> - The type of object that is to be created</li>
    <li><b>metadata</b> - Metadata including the name, namespace, and labels to identify the resource</li>
    <li><b>spec</b> - The specification of the object</li>
</ul>
<p>
    This allows us to create and destroy resources within a k8s cluster.
    Now let's have a closer look at the types of resources we can create.
</p>

<h3>Pods</h3>
<p>
    <b>Pods</b> are the smallest unit of scheduling in k8s. They
    can contain multiple tighly coupled containers, that share all resources.
    This enables containers in the same pod to communicate as if they
    are colocated on the same logical host. This colocation imples the access
    to the same port ranges, memory, and file access.
</p>
<p>
    The following example describes a pod including a single nginx container
    exposing port 80 to the other pods.
</p>
<pre><code class="less">apiVersion: v1
kind: Pod
metadata:
    name: nginx
    labels:
        app: srv
spec:
    containers:
    - name: nginx
      image: nginx:1.7.9
      ports:
      - containerPort: 80
</code></pre>
<p>
    Manually deploying pods to k8s clusters is not common practice. <b>Deployments</b> 
    that create <b>ReplicationControllers</b>, which make sure that there is a certain 
    number of pods running at a certain time, handle the life cycle of pods.
</p>

<h3>Deployments</h3>
<p>
    <b>Deployments</b> exist to regulate the creation, destruction, and life cycle 
    management of pods. A deployment depicts the target state of the application. 
    If a new version of an application gets deployed to a k8s cluster, deployments 
    take care of the bureaucracy of spinning up new pods and fading out old ones 
    without affecting uptime.
</p>
<p>
    The following deployment spins up 3 replicas of a nginx container
</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
    name: srv-deploy
    labels:
        app: srv
spec:
    replicas: 3
    selector:
        matchLabels:
            app: srv
    template:
        metadata:
            labels:
                app: srv
        spec:
            containers:
            - name: nginx
              image: nginx:1.7.9
              ports:
              - containerPort: 80
</code></pre>
<p>
    The <b>template</b> field of the deployment specification follows the exact syntax of 
    a pod definition. If the manifest is changed and applied to the cluster, in the background 
    k8s spins up another deployment, waiting for the containers to be available, then 
    gradually removes the old pods.
</p>
<p>
    Up and down scaling of the replicas is handled by a <b>ReplicationController</b> (RC). It's
    job is to make sure that a specified number of replicas are up and
    running at any given moment. If a pod crashes or becomes unresponsive, the RC will terminate
    the dying pod and spawn another one as a replacement.
</p>
<p>
    When creating multiple replicas of a pod, we need an abstraction to handle access in a 
    unified way. <b>Services</b> provide exactly that abstraction.
</p>

<h3>Services</h3>
<p>
    <b>Services</b> tie a group of pods together to a unified endpoint by using
    label selectors. They also represent addressable named units within a 
    namespace.
</p>
<p>
    The following manifest shows a service that selects all pods with the 
    <b>app: srv</b> label and then exposes port 80.
</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
    name: somesvc
spec:
    selector:
        app: srv
    ports:
    - protocol: TCP
      port: 80
</code></pre>
<p>
    The service we created is limited to the local k8s network
    and currently isn't exposed via a public IP. To expose services
    to the internet we create a resource called <b>ingress</b>.
</p>

<h3>Ingress</h3>
<p>
    To fulfill the promise of <em>multitenancy</em>, a k8s cluster needs to
    be able to route to multiple endpoints independently. This is where <b>ingress</b> 
    comes in, as the name already implies ingress has the ability of reverse
    proxying from the internet into the cluster. 
</p>
<p>
    The following example routes <b>s1.example.com</b> to the service 
    <b>s1</b> and <b>s2.example.com</b> to <b>s2</b>.
</p>
<pre><code class="less">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
    name: test
spec:
    rules:
    - host: s1.example.com
    http:
        paths:
        - backend:
            serviceName: s1
            servicePort: 80
    - host: s2.example.com
    http:
        paths:
        - backend:
            serviceName: s2
            servicePort: 80
</code></pre>
<p>
    Ingress, being the primary endpoint for any incoming connection, 
    also supports <em>TLS</em>.
</p>
<p>
    Now we have a way of deploying our services, scaling them independently, 
    and routing to them from the internet. This covers the very basics of k8s.
</p>

<h3>Conclusion</h3>
<p>
    Kubernetes is a powerful orchestration tool. With the above mentioned resources we can
    already automate a service to run in our cluster. 
</p>
<p>
    Here a few best practices to adhere by when dealing with k8s resources
</p>
<ul>
    <li>Never deploy pods on their own - always use <b>deployments</b> to manage their lifecycle</li>
    <li>
        Use <em>one container per pod</em> - this avoids port collisions and keeps your 
        containers independently scalable
    </li>
    <li>Only use the <b>latest</b> tag for internal docker images, never for external ones</li>
    <li>
        Create services before deployments are created
        <ul>
            <li>Deployments take some time to ramp up</li>
            <li>Services are created instantly</li>
        </ul>
    </li>
    <li>Use <b>ingress</b> to expose services to the outside world instead of <b>services</b></li>
    <li>Include the orchestration logic in the service's version control system</li>
    <li>Create a CICD pipeline that deploys to your cluster in an automated fashion</li>
</ul>
<p>
    There are additional concepts of vital importance to a k8s cluster. The
    following is a non-exhaustive list of additional <em>kinds</em> to explore
</p>
<ul>
    <li>
        <b>Namespace</b> - Used, among other things, to avoid naming conflicts when 
        dealing with multiple environments
    </li>
    <li>
        <b>ConfigMap</b> and <b>Secret</b> - Introduce potentially shared static configuration to the pods
    </li>
    <li>
        <b>StorageClass</b> and <b>PersistentVolumeClaim</b> - Used to share volatile data
        among pods
    </li>
    <li>
        <b>NetworkPolicy</b> - Used to regulate egress and ingress access to certain nodes 
        and services
    </li>
    <li>
        <b>Job</b> and <b>CronJob</b> - Used to run single or periodic jobs on a k8s cluster
    </li>
</ul>
<p>
    For more information, refer to the k8s documentation<sup>5</sup>
</p>
<hr>
<h3>References</h3>
<ol class="references">
    <li>
        <a target="_blank" href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">
            Kubernetes resources on <b>Pods</b>
        </a>
    </li>
    <li>
        <a target="_blank" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">
            Kubernetes resources on <b>Deployments</b>
        </a>
    </li>
    <li>
        <a target="_blank" href="https://kubernetes.io/docs/concepts/services-networking/service/">
            Kubernetes resources on <b>Services</b>
        </a>
    </li>
    <li>
        <a target="_blank" href="https://kubernetes.io/docs/concepts/services-networking/ingress/">
            Kubernetes resources on <b>Ingress</b>
        </a>
    </li>
    <li>
        <a target="_blank" href="https://kubernetes.io/docs/home/">
            Kubernetes documentation
        </a>
    </li>
</ol>